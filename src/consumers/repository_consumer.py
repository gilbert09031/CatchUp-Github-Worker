from faststream.rabbit import RabbitRouter
from src.models.github import GithubRepoSyncRequest
from src.models.search import GithubCodeDocument
from src.services.github_client_adaptive import GithubClientAdaptive  # Adaptive ë°©ì‹ ì‚¬ìš©
from src.chunking.code_chunker import CodeChunker
from src.embedding.openai_embedder import OpenAIEmbedder
from src.indexing.meili_indexer import MeiliIndexer
from src.config.settings import get_settings
import logging

router = RabbitRouter()
logger = logging.getLogger(__name__)
settings = get_settings()

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë“¤ ì´ˆê¸°í™” (ì‹±ê¸€í†¤ì²˜ëŸ¼ ì¬ì‚¬ìš©)
chunker = CodeChunker()
embedder = OpenAIEmbedder()


@router.subscriber("github_repository_queue")
async def sync_repository_code(msg: GithubRepoSyncRequest):
    logger.info(f"ğŸ”„ Processing: {msg.owner}/{msg.repo_name} (Branch: {msg.branch})")

    try:
        # ì €ì¥ì†Œë³„ ì¸ë±ìŠ¤ ì´ë¦„ ìƒì„±
        index_name = MeiliIndexer.get_index_name(msg.repo_name, msg.branch)
        logger.info(f"Using index: {index_name}")

        # ì €ì¥ì†Œë³„ Indexer ìƒì„±
        indexer = MeiliIndexer(index_name=index_name)

        # Adaptive Client: ZIP í¬ê¸°ì— ë”°ë¼ ìë™ìœ¼ë¡œ Hybrid ë˜ëŠ” Tree API ì„ íƒ
        client = GithubClientAdaptive(
            token=msg.github_token,
            max_zip_size_mb=settings.MAX_ZIP_SIZE_MB
        )

        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë²„í¼
        doc_buffer = []
        BATCH_SIZE = 20  # ì„ë² ë”©/ì¸ë±ì‹±ì„ í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜
        total_processed = 0

        async for file_obj in client.fetch_repo_files(
            msg.owner,
            msg.repo_name,
            msg.branch
        ):
            # 1. Chunking
            chunks = chunker.chunk_file(
                file_obj,
                msg.repository_id
            )

            for i, chunk in enumerate(chunks):
                # 2. Document ë³€í™˜ (metadata í¬í•¨)
                doc = GithubCodeDocument(
                    id=GithubCodeDocument.generate_id(msg.repository_id, chunk.file_path, i),
                    file_path=chunk.file_path,
                    category="CODE",
                    source=f"{msg.repo_name}@{msg.branch}",
                    text=chunk.content,
                    repository_id=msg.repository_id,
                    owner=msg.owner,
                    language=chunk.language,
                    html_url=f"https://github.com/{msg.owner}/{msg.repo_name}/blob/{msg.branch}/{chunk.file_path}",
                    metadata=chunk.metadata,  # metadata ì „ë‹¬
                    _vectors={}
                )
                doc_buffer.append(doc)

            # 3. ë²„í¼ê°€ ì°¨ë©´ ë°°ì¹˜ ì²˜ë¦¬ (Embedding -> Indexing)
            if len(doc_buffer) >= BATCH_SIZE:
                await process_batch(doc_buffer, indexer)
                total_processed += len(doc_buffer)
                doc_buffer = []  # ë²„í¼ ì´ˆê¸°í™”

        # 4. ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
        if doc_buffer:
            await process_batch(doc_buffer, indexer)
            total_processed += len(doc_buffer)

        logger.info(f"Sync Complete | Index: {index_name}, Total Documents: {total_processed}")

    except Exception as e:
        logger.error(f"Failed to sync repository: {e}")
        # Adaptive ClientëŠ” ìë™ìœ¼ë¡œ í´ë°±í•˜ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”


async def process_batch(docs: list[GithubCodeDocument], indexer: MeiliIndexer):
    """
    ë¬¸ì„œ ë°°ì¹˜ë¥¼ ì„ë² ë”©í•˜ê³  ì¸ë±ì‹±í•©ë‹ˆë‹¤.

    Args:
        docs: ì²˜ë¦¬í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        indexer: ì‚¬ìš©í•  MeiliIndexer ì¸ìŠ¤í„´ìŠ¤
    """
    texts = [d.text for d in docs]
    embeddings = await embedder.embed_documents(texts)

    ready_docs = []
    for doc, vector in zip(docs, embeddings):
        # ë²¡í„°ë¥¼ "default" í‚¤ë¥¼ ê°€ì§„ ë”•ì…”ë„ˆë¦¬ë¡œ ê°ìŒ‰ë‹ˆë‹¤
        doc.vectors = {"default": vector}
        ready_docs.append(doc.model_dump(by_alias=True))

    await indexer.add_documents(ready_docs)