from faststream.rabbit import RabbitRouter
from src.models.github import GithubRepoSyncRequest
from src.models.search import GithubCodeDocument
from src.services.github_client_adaptive import GithubClientAdaptive  # Adaptive Î∞©Ïãù ÏÇ¨Ïö©
from src.chunking.code_chunker import CodeChunker
from src.indexing.meili_indexer import MeiliIndexer
from src.config.settings import get_settings
from src.utils.file_utils import get_file_category
import logging

router = RabbitRouter()
logger = logging.getLogger(__name__)
settings = get_settings()

# ÏÑúÎπÑÏä§ Ïù∏Ïä§ÌÑ¥Ïä§Îì§ Ï¥àÍ∏∞Ìôî (Ïã±Í∏ÄÌÜ§Ï≤òÎüº Ïû¨ÏÇ¨Ïö©)
chunker = CodeChunker()


@router.subscriber("github_repository_queue")
async def sync_repository_code(msg: GithubRepoSyncRequest):
    logger.info(f"üîÑ Processing: {msg.owner}/{msg.repo_name} (Branch: {msg.branch})")

    try:
        # Ï†ÄÏû•ÏÜåÎ≥Ñ Ïù∏Îç±Ïä§ Ïù¥Î¶Ñ ÏÉùÏÑ±
        index_name = MeiliIndexer.get_index_name(msg.repo_name, msg.branch) + "_code"
        logger.info(f"Using index: {index_name}")

        # Ï†ÄÏû•ÏÜåÎ≥Ñ Indexer ÏÉùÏÑ±
        indexer = MeiliIndexer(index_name=index_name)

        # Adaptive Client: ZIP ÌÅ¨Í∏∞Ïóê Îî∞Îùº ÏûêÎèôÏúºÎ°ú Hybrid ÎòêÎäî Tree API ÏÑ†ÌÉù
        client = GithubClientAdaptive(
            token=msg.github_token,
            max_zip_size_mb=settings.MAX_ZIP_SIZE_MB
        )

        # Î∞∞Ïπò Ï≤òÎ¶¨Î•º ÏúÑÌïú Î≤ÑÌçº
        doc_buffer = []
        BATCH_SIZE = 100
        total_processed = 0

        async for file_obj in client.fetch_repo_files(
            msg.owner,
            msg.repo_name,
            msg.branch
        ):
            # 1. Chunking
            chunks = chunker.chunk_file(
                file_obj,
                msg.repository_id
            )

            for i, chunk in enumerate(chunks):
                # 2. Document Î≥ÄÌôò (metadata Ìè¨Ìï®)
                doc = GithubCodeDocument(
                    # Meta
                    source_type = 0,
                    # Primary Key
                    id = GithubCodeDocument.generate_id(chunk.file_path, i),
                    # Repository Ï†ïÎ≥¥
                    owner = msg.owner,
                    repo = msg.repo_name,
                    branch = msg.branch,
                    # File Ï†ïÎ≥¥
                    file_path = chunk.file_path,
                    chunk_number = i,
                    category = get_file_category(chunk.file_path, chunk.language),
                    # Content
                    text = chunk.content,
                    metadata = chunk.metadata,
                    # Link
                    html_url = f"https://github.com/{msg.owner}/{msg.repo_name}/blob/{msg.branch}/{chunk.file_path}"
                )
                doc_buffer.append(doc)

            # 3. Î≤ÑÌçºÍ∞Ä Ï∞®Î©¥ Î∞∞Ïπò Ïù∏Îç±Ïã±
            if len(doc_buffer) >= BATCH_SIZE:
                ready_docs = [doc.model_dump(by_alias=True) for doc in doc_buffer]
                await indexer.add_documents(ready_docs)
                total_processed += len(doc_buffer)
                doc_buffer = []  # Î≤ÑÌçº Ï¥àÍ∏∞Ìôî

        # 4. ÎÇ®ÏùÄ Î≤ÑÌçº Ï≤òÎ¶¨
        if doc_buffer:
            ready_docs = [doc.model_dump(by_alias=True) for doc in doc_buffer]
            await indexer.add_documents(ready_docs)
            total_processed += len(doc_buffer)

        logger.info(f"Sync Complete | Index: {index_name}, Total Documents: {total_processed}")

    except Exception as e:
        logger.error(f"Failed to sync repository: {e}")